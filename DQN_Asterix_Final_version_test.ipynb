{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 3s (432 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting gymnasium[atari]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.3.2-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[atari])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[atari]) (6.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 shimmy-0.2.1 stable-baselines3-2.3.2\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2024.6.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446663 sha256=0fb6828c3f0b9df6e44153e09f124fce2e4b194afb344b7b8b0ca72a885ee62a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "# !apt-get install -y swig\n",
        "# !pip install gymnasium[atari] torch torchvision stable-baselines3\n",
        "# !pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhjUp-wW7zx6"
      },
      "source": [
        "## Import Dependencies ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fwaosFMR7zx8"
      },
      "outputs": [],
      "source": [
        "## Import Dependencies ##\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import DQN  # Import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.atari_wrappers import ClipRewardEnv\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "\n",
        "# Enable CuDNN benchmark mode\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjxJkZ9-9qJu",
        "outputId": "a159245e-059c-45d4-edf6-eec69e67c27e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzvKLbAm7zx-"
      },
      "source": [
        "# Custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VZ_V4c847zx-"
      },
      "outputs": [],
      "source": [
        "# Custom CNN feature extractor\n",
        "class CustomCNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "class CustomCNNDeep(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNNDeep, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "\n",
        "class CustomCNNWide(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNNWide, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 64, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n",
        "\n",
        "class CustomCNNShallow(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
        "        super(CustomCNNShallow, self).__init__(observation_space, features_dim)\n",
        "        n_input_channels = observation_space.shape[0]\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FqkTepR7zx_"
      },
      "source": [
        "# Configuration of DQN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C39jx4fY7zx_"
      },
      "outputs": [],
      "source": [
        "## Configuration dictionary ##\n",
        "config = {\n",
        "    'learning_rate': 1e-4,\n",
        "    'buffer_size': 1000000, #replay memory size\n",
        "    'learning_starts': 100,\n",
        "    'batch_size': 32,\n",
        "    'tau': 1.0, #soft update coefficient 1 is hard update\n",
        "    'gamma': 0.99, #discount factor\n",
        "    'train_freq': 4,\n",
        "    'gradient_steps': 1,\n",
        "    'target_update_interval': 10000,\n",
        "    'exploration_fraction': 0.1, #decay\n",
        "    'exploration_initial_eps': 1.0,\n",
        "    'exploration_final_eps': 0.05,\n",
        "    'max_grad_norm': 10,\n",
        "    'replay_buffer_class': None,\n",
        "    'replay_buffer_kwargs': None,\n",
        "    'optimize_memory_usage': False,\n",
        "    'stats_window_size': 100,\n",
        "    'tensorboard_log': None,\n",
        "    'policy_kwargs': None, #neural network size\n",
        "    'verbose': 0,\n",
        "    'seed': None,\n",
        "    'device': 'auto',\n",
        "    '_init_setup_model': True\n",
        "}\n",
        "\n",
        "# Define the log path using os.path.join for cross-platform compatibility\n",
        "log_path = os.path.join('Training', 'Target_update_Logs')\n",
        "\n",
        "# Update parameters based on your preferred values\n",
        "config.update({\n",
        "    'learning_rate': 1e-4,              #1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-8\n",
        "    'buffer_size': 75000,               #25000, 50000, 75000, 100000\n",
        "    'batch_size': 64,                   #16, 32, 64, 128\n",
        "    'gamma': 0.99,                      #0.999, 0.99, 0.95\n",
        "    'target_update_interval': 20000,    #5000, 10000, 20000, 30000\n",
        "    'exploration_fraction': 0.1,        #0.1, 0.2, 0.5, 0.99\n",
        "    'exploration_initial_eps': 1.0,     #1.0, 0.9\n",
        "    'exploration_final_eps': 0.05,      #0.1, 0.05, 0.01\n",
        "\n",
        "    'policy_kwargs': {\n",
        "         'features_extractor_class': CustomCNN,\n",
        "         'features_extractor_kwargs': {'features_dim': 128},\n",
        "         'net_arch': [256, 256],\n",
        "         'activation_fn': th.nn.ReLU\n",
        "     },\n",
        "    'verbose': 1,\n",
        "    # 'tensorboard_log': '/content/drive/MyDrive/Colab Notebooks/Asterix/Training/Logs_Asterix/200k_Logs', ## change for different logs different parameters\n",
        "    'tensorboard_log': log_path, ## change for different logs different parameters\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrNmL7CR7zyA"
      },
      "source": [
        "## Make Environment ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XgNmgla57zyA"
      },
      "outputs": [],
      "source": [
        "## Preprocessing and Wrappers ##\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
        "    env.metadata['render_fps'] = 10000 # Disable FPS limit for faster rendering\n",
        "    env = Monitor(env)  # Monitor to keep track of rewards and episode lengths\n",
        "    env = gym.wrappers.AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
        "    # env = ClipRewardEnv(env)  # Add reward clipping\n",
        "    env = gym.wrappers.FrameStack(env, 4)  # Stack frames to provide temporal context\n",
        "    return env\n",
        "\n",
        "## Test Environment ##\n",
        "environment_name = 'AsterixNoFrameskip-v4'\n",
        "env = make_env(environment_name)\n",
        "env = DummyVecEnv([lambda: env])\n",
        "env = VecFrameStack(env, n_stack=4)  # Ensure the frames are stacked\n",
        "obs = env.reset()\n",
        "# env.action_space\n",
        "# env.observation_space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5JQVXKh7zyB"
      },
      "source": [
        "## Test Environment ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpHMaAk37zyB",
        "outputId": "b54eaa9d-8314-4e26-fa01-ab49d6d582c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:1 Score:150.0\n",
            "Episode:2 Score:300.0\n",
            "Episode:3 Score:250.0\n",
            "Episode:4 Score:100.0\n",
            "Episode:5 Score:50.0\n"
          ]
        }
      ],
      "source": [
        "episodes = 5\n",
        "\n",
        "for episode in range(1, episodes+1):\n",
        "    obs = env.reset()\n",
        "    done = [False]\n",
        "    score = 0\n",
        "\n",
        "    while not done[0]:\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        obs, rewards, dones, infos = env.step([action])  # Wrap action in list for DummyVecEnv\n",
        "        score += rewards[0]\n",
        "\n",
        "        done = dones  # done is already a list\n",
        "    print('Episode:{} Score:{}'.format(episode, score))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkhekS8b7zyC"
      },
      "source": [
        "## Train Model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA5H7RPn7zyC",
        "outputId": "a907aadf-87a5-4082-b08d-c64ff6f2756f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:241: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 16.94GB > 7.62GB\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# First Training Session\n",
        "# log_path = os.path.join('Training', 'Target_update_Logs')\n",
        "# model = DQN('CnnPolicy', env, verbose=1, tensorboard_log=log_path, buffer_size=10000, batch_size=32)\n",
        "model = DQN('CnnPolicy', env, **config)  # Enable Double DQN -- double_q=True\n",
        "\n",
        "\n",
        "# Recreate callbacks to ensure they are properly initialized\n",
        "# checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./drive/MyDrive/Colab Notebooks/Asterix/logs/', name_prefix='dqn_model')\n",
        "# eval_callback = EvalCallback(env, best_model_save_path='./drive/MyDrive/Colab Notebooks/Asterix/logs/best_model', log_path='./drive/MyDrive/Colab Notebooks/Asterix/logs/results', eval_freq=500, deterministic=True, render=False)\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/', name_prefix='dqn_model')\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model', log_path='./logs/results', eval_freq=500, deterministic=True, render=False)\n",
        "\n",
        "# CNN tuning\n",
        "# tb_log_name = \"CNN_Base\"\n",
        "# tb_log_name = \"CNN_Custom\"\n",
        "# tb_log_name = \"CNN_Deep\"\n",
        "# tb_log_name = \"CNN_Wide\"\n",
        "# tb_log_name = \"CNN_Shalow\"\n",
        "\n",
        "# Lr tuning\n",
        "# tb_log_name = \"lr_e8\"\n",
        "# tb_log_name = \"lr_e6\"\n",
        "# tb_log_name = \"lr_e5\"\n",
        "# tb_log_name = \"lr_e4\"\n",
        "# tb_log_name = \"lr_e3\"\n",
        "# tb_log_name = \"lr_e2\"\n",
        "\n",
        "# exploration frac tuning\n",
        "# tb_log_name = \"expl_Frac_0.1\"\n",
        "# tb_log_name = \"expl_Frac_0.2\"\n",
        "# tb_log_name = \"expl_Frac_0.5\"\n",
        "# tb_log_name = \"expl_Frac_0.99\"\n",
        "\n",
        "# exploration final tuning\n",
        "# tb_log_name = \"expl_final_0.01\"\n",
        "# tb_log_name = \"expl_final_0.05\"\n",
        "# tb_log_name = \"expl_final_0.1\"\n",
        "\n",
        "# target update tuning\n",
        "# tb_log_name = \"target_up_30k\"\n",
        "# tb_log_name = \"target_up_10k\"\n",
        "\n",
        "\n",
        "# --> used for trianing final model\n",
        "\n",
        "tb_log_name = \"target_up_20k\"\n",
        "\n",
        "# exploration init tuning\n",
        "# tb_log_name = \"expl_init_0.9\"\n",
        "# tb_log_name = \"expl_init_0.95\"\n",
        "# tb_log_name = \"expl_init_1.0\"\n",
        "\n",
        "# gamma tuning\n",
        "# tb_log_name = \"gamma_0.95\"\n",
        "# tb_log_name = \"gamma_0.99\"\n",
        "# tb_log_name = \"gamma_0.999\"\n",
        "\n",
        "# tb_log_name = \"200k\"\n",
        "# tb_log_name = \"Assault_200k_2\"\n",
        "# tb_log_name = \"Assault_Lr_e2\"\n",
        "# tb_log_name = \"SpaceInvaders_200k\"\n",
        "# tb_log_name = \"SpaceInvaders_basic\"\n",
        "# tb_log_name = \"SpaceInvaders_Lr_e2\"\n",
        "# tb_log_name = \"SpaceInvaders_Lr_e2_frac_0.3\"\n",
        "# tb_log_name = \"Breakout_200k\"\n",
        "# tb_log_name = \"Asterix_Test\"\n",
        "# tb_log_name = \"Asterix_Test2\"\n",
        "\n",
        "# saved_model = \"Model_Assualt_200k_2\"\n",
        "# saved_model = \"Model_Assualt_Lr_e2\"\n",
        "# saved_model = \"Model_SpaceInvaders_200k\"\n",
        "# saved_model = \"Model_SpaceInvaders_basic\"\n",
        "# saved_model = \"Model_SpaceInvaders_Lr_e2\"\n",
        "# saved_model = \"Model_SpaceInvaders_Lr_e2_frac_0.3\"\n",
        "# saved_model = \"Model_Breakout_200k\"\n",
        "# saved_model = \"Model_Asterix_Test\"\n",
        "# saved_model = \"Model_Asterix_Test2\"\n",
        "# saved_model = \"Model_200k\"\n",
        "\n",
        "# gamma tuning\n",
        "# saved_model = \"Model_gamma_0.95\"\n",
        "# saved_model = \"Model_gamma_0.99\"\n",
        "# saved_model = \"Model_gamma_0.999\"\n",
        "\n",
        "# exploration init tuning\n",
        "# saved_model = \"Model_expl_init_0.9\"\n",
        "# saved_model = \"Model_expl_init_0.95\"\n",
        "# saved_model = \"Model_expl_init_1.0\"\n",
        "\n",
        "# target update tuning\n",
        "# saved_model = \"Model_target_up_30k\"\n",
        "# saved_model = \"Model_target_up_10k\"\n",
        "\n",
        "\n",
        "# --> used for training final model\n",
        "\n",
        "saved_model = \"Model_target_up_20k\"\n",
        "\n",
        "# exploration final tuning\n",
        "# saved_model = \"Model_expl_final_0.01\"\n",
        "# saved_model = \"Model_expl_final_0.05\"\n",
        "# saved_model = \"Model_expl_final_0.1\"\n",
        "\n",
        "# exploration frac tuning\n",
        "# saved_model = \"Model_expl_frac_0.1\"\n",
        "# saved_model = \"Model_expl_frac_0.2\"\n",
        "# saved_model = \"Model_expl_frac_0.5\"\n",
        "# saved_model = \"Model_expl_frac_0.99\"\n",
        "\n",
        "\n",
        "# Buffer Tuning\n",
        "# tb_log_name = \"Buffer_25k\"\n",
        "# tb_log_name = \"Buffer_50k\"\n",
        "# tb_log_name = \"Buffer_100k\"\n",
        "# tb_log_name = \"Buffer_75k\"\n",
        "# tb_log_name = \"Buffer_500k\"\n",
        "\n",
        "# saved_model = \"Model_buffer_25k\"\n",
        "# saved_model = \"Model_buffer_50k\"\n",
        "# saved_model = \"Model_buffer_100k\"\n",
        "# saved_model = \"Model_buffer_75k\"\n",
        "# saved_model = \"Model_buffer_500k\"\n",
        "\n",
        "# Batch tuning\n",
        "# tb_log_name = \"Batch_16\"\n",
        "# tb_log_name = \"Batch_32\"\n",
        "# tb_log_name = \"Batch_64\"\n",
        "# tb_log_name = \"Batch_128\"\n",
        "\n",
        "# Batch tuning\n",
        "# saved_model = \"Model_Batch_16\"\n",
        "# saved_model = \"Model_Batch_32\"\n",
        "# saved_model = \"Model_Batch_64\"\n",
        "# saved_model = \"Model_Batch_128\"\n",
        "\n",
        "# CNN Tuning\n",
        "#saved_model = \"Model_CNN_Base\"\n",
        "# saved_model = \"Model_CNN_Custom\"\n",
        "# saved_model = \"Model_CNN_Deep\"\n",
        "# saved_model = \"Model_CNN_Wide\"\n",
        "# saved_model = \"Model_CNN_Shalow\"\n",
        "\n",
        "# Lr tuning\n",
        "# saved_model = \"Model_lr_e8\"\n",
        "# saved_model = \"Model_lr_e6\"\n",
        "# saved_model = \"Model_lr_2_e5\"\n",
        "# saved_model = \"Model_lr_2_e4\"\n",
        "# saved_model = \"Model_lr_e3\"\n",
        "# saved_model = \"Model_lr_e2\"\n",
        "\n",
        "# Enable CuDNN benchmark mode\n",
        "\n",
        "# Train for initial steps\n",
        "# model.learn(total_timesteps=2000, callback=[checkpoint_callback, eval_callback], reset_num_timesteps=False)\n",
        "#\n",
        "# model.learn(total_timesteps=50000, tb_log_name=tb_log_name, callback=[checkpoint_callback, eval_callback])\n",
        "# Pass reset_num_timesteps=False to continue the training curve in tensorboard\n",
        "# By default, it will create a new curve\n",
        "# model.learn(total_timesteps=2000, tb_log_name=\"second_run\", callback=[checkpoint_callback, eval_callback], reset_num_timesteps=False)\n",
        "# model.learn(total_timesteps=2000, tb_log_name=\"third_run\", callback=[checkpoint_callback, eval_callback], reset_num_timesteps=False)\n",
        "# Assault: 2m 18.2 s / 2m 44 s / 3.45 s\n",
        "# Asterix: 2m 37.3 /1.47.2 /1.39  10000: 14.45.7 m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTjCHPv7zyD"
      },
      "source": [
        "## Save and Reload Model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MnMmxZwr7zyD"
      },
      "outputs": [],
      "source": [
        "## Save and Reload Model ##\n",
        "dqn_path = os.path.join('Training', 'Saved_Models', saved_model)\n",
        "\n",
        "# dqn_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/Asterix/Training/Saved_Models', saved_model)\n",
        "# model.save(dqn_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W376YY3X7zyD"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "dqn_path = os.path.join('Training', 'Saved_Models', saved_model)\n",
        "# dqn_path = os.path.join('/content/drive/MyDrive/Colab Notebooks/Asterix/Training/Saved_Models', saved_model)\n",
        "model = DQN.load(dqn_path, env)\n",
        "\n",
        "# Recreate callbacks to ensure they are properly initialized\n",
        "# checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./drive/MyDrive/Colab Notebooks/Asterix/logs/', name_prefix='dqn_model')\n",
        "# eval_callback = EvalCallback(env, best_model_save_path='./drive/MyDrive/Colab Notebooks/Asterix/logs/best_model', log_path='./drive/MyDrive/Colab Notebooks/Asterix/logs/results', eval_freq=500, deterministic=True, render=False)\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/', name_prefix='dqn_model')\n",
        "eval_callback = EvalCallback(env, best_model_save_path='./logs/best_model', log_path='./logs/results', eval_freq=500, deterministic=True, render=False)\n",
        "\n",
        "model.set_env(env)\n",
        "# Continue training for more steps\n",
        "model.learn(total_timesteps=50000, tb_log_name=tb_log_name, callback=[checkpoint_callback, eval_callback], reset_num_timesteps=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuf2FeMW7zyE",
        "outputId": "1619832d-b5cc-4c7c-aaf0-91de5696a4e1"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Training\\\\Saved_Models\\\\Model_target_up_20k.zip'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#save final model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# dqn_path_1 = os.path.join('Training', 'Saved Models', 'DQN_BeamRider_Model_1')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model.save(dqn_path)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# del model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN\u001b[38;5;241m.\u001b[39mload(dqn_path, env)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:680\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    678\u001b[0m     get_system_info()\n\u001b[1;32m--> 680\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m load_from_zip_file(\n\u001b[0;32m    681\u001b[0m     path,\n\u001b[0;32m    682\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    683\u001b[0m     custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    684\u001b[0m     print_system_info\u001b[38;5;241m=\u001b[39mprint_system_info,\n\u001b[0;32m    685\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:403\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[1;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_zip_file\u001b[39m(\n\u001b[0;32m    377\u001b[0m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath, io\u001b[38;5;241m.\u001b[39mBufferedIOBase],\n\u001b[0;32m    378\u001b[0m     load_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     print_system_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     file \u001b[38;5;241m=\u001b[39m open_path(load_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39mverbose, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device(device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:240\u001b[0m, in \u001b[0;36mopen_path_str\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;129m@open_path\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedIOBase:\n\u001b[0;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    that the path exists.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m open_path_pathlib(pathlib\u001b[38;5;241m.\u001b[39mPath(path), mode, verbose, suffix)\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:291\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    285\u001b[0m         path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m open_path_pathlib(path, mode, verbose, suffix)\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:272\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    270\u001b[0m             path, suffix \u001b[38;5;241m=\u001b[39m newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 272\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[1;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(path\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m), mode, verbose, suffix)\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\djang\\anaconda3\\envs\\Python311\\Lib\\pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Training\\\\Saved_Models\\\\Model_target_up_20k.zip'"
          ]
        }
      ],
      "source": [
        "#save final model\n",
        "# dqn_path_1 = os.path.join('Training', 'Saved Models', 'DQN_BeamRider_Model_1')\n",
        "# model.save(dqn_path)\n",
        "# del model\n",
        "model = DQN.load(dqn_path, env)\n",
        "model.set_env(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzHheXux7zyE"
      },
      "source": [
        "## Evaluate and Test ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu2ugL9u7zyE",
        "outputId": "2b3ec419-56e7-4b76-b23b-cc7808b2f639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Episode:1 Score:1450.0\n",
            "Evaluation Episode:2 Score:1100.0\n",
            "Evaluation Episode:3 Score:1500.0\n",
            "Evaluation Episode:4 Score:1500.0\n",
            "Evaluation Episode:5 Score:1100.0\n"
          ]
        }
      ],
      "source": [
        "## Evaluate and Test ##\n",
        "# evaluate_policy(model, env, n_eval_episodes=10, render=False)\n",
        "# env.close()\n",
        "\n",
        "# ## Evaluate and Test ##\n",
        "episodes = 5\n",
        "for episode in range(1, episodes + 1):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = model.predict(obs, deterministic=True)[0]  # Use model to predict action\n",
        "        obs, reward, done, info = env.step(action)  # Wrap action in list for DummyVecEnv\n",
        "        score += reward[0]\n",
        "\n",
        "        # done = terminated[0] or truncated[0]  # Correctly handle the vectorized done flag\n",
        "    print('Evaluation Episode:{} Score:{}'.format(episode, score))\n",
        "\n",
        "env.close()\n",
        "\n",
        "## Visualization using TensorBoard ##\n",
        "# In your terminal, run: tensorboard --logdir=./logs/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "pC-XpWI7CABa",
        "outputId": "f8e5f89c-32f4-4eec-c4eb-28f94b2ac3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: kill: (14898) - No such process\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 2).\n",
              "Contents of stderr:\n",
              "2024-06-15 18:36:32.018864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
              "2024-06-15 18:36:32.018938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
              "2024-06-15 18:36:32.020318: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
              "2024-06-15 18:36:33.069501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
              "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC] [--host ADDR]\n",
              "                   [--bind_all] [--port PORT] [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
              "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
              "                   [--grpc_creds_type {local,ssl,ssl_dev}] [--grpc_data_provider PORT]\n",
              "                   [--purge_orphaned_data BOOL] [--db URI] [--db_import] [--inspect]\n",
              "                   [--version_tb] [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
              "                   [--window_title TEXT] [--max_reload_threads COUNT] [--reload_interval SECONDS]\n",
              "                   [--reload_task TYPE] [--reload_multifile BOOL]\n",
              "                   [--reload_multifile_inactive_secs SECONDS] [--generic_data TYPE]\n",
              "                   [--samples_per_plugin SAMPLES_PER_PLUGIN] [--detect_file_replacement BOOL]\n",
              "                   {serve,dev} ...\n",
              "tensorboard: error: argument {serve,dev}: invalid choice: 'Notebooks/Asterix/Training/Logs_Asterix/CNN_Logs/' (choose from 'serve', 'dev')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# !kill 14898\n",
        "# %load_ext tensorboard\n",
        "# pathoflogsdir = \"/content/drive/MyDrive/Colab Notebooks/Asterix/Training/Logs_Asterix/CNN_Logs/\"\n",
        "# %tensorboard --logdir $pathoflogsdir"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
